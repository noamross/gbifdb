---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE,
  fig.path = "man/figures/README-",
  out.width = "100%"
)


Sys.setenv("GBIF_HOME"="/minio/shared-data/occurrence.parquet")

```

# gbifdb

<!-- badges: start -->
[![R-CMD-check](https://github.com/cboettig/gbifdb/workflows/R-CMD-check/badge.svg)](https://github.com/cboettig/gbifdb/actions)
<!-- badges: end -->

The goal of `gbifdb` is to provide a relational database interface to a `parquet` based serializations of `gbif` data. 
Instead of requiring custom functions for filtering and selecting data from the central GBIF server (as in `rgbif`), `gbifdb` users can take advantage of the full array of `dplyr` and `tidyr` functions which can be automatically translated to SQL by `dbplyr`.
Users already familiar with SQL can construct SQL queries directly with `DBI` instead. 
`gbifdb` sends these queries to [`duckdb`](https://duckdb.org), a high-performance, columnar-oriented database engine which runs entirely inside the client,
(unlike server-client databases such as MySQL or Postgres, no additional setup is needed outside of installing `gbifdb`.)
`duckdb` is able to execute these SQL queries directly on-disk against the Parquet data files, side-stepping limitations of available RAM or the need to import the data. 
It's highly optimized implementation can be faster even than in-memory operations in `dplyr`, especially when using fast SSD-based storage disks.
Unlike the `arrow` R packae interface, which can also query `parquet` files, `duckdb` supports the full set of SQL instructions, including windowed operations like `group_by`+`summarise` as well as table joins.


## Installation

**NOTE**: `gbifdb` currently requires the dev version of `duckdb`, which you can install using:

```r
install.packages("https://github.com/duckdb/duckdb/releases/download/master-builds/duckdb_r_src.tar.gz", repos = NULL, type = "source")
```

<!--

You can install the released version of `gbifdb` from [CRAN](https://CRAN.R-project.org) with:

``` r
install.packages("gbifdb")
```

-->

And the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("cboettig/gbifdb")
```

`gbifdb` has few dependencies: only `duckdb` and `DBI` are required.  

## Getting Started

```{r message=FALSE}
library(gbifdb)
library(dplyr)  # optional, for dplyr-based operations
```

Before you can use `gbifdb` you will need to download GBIF data.
Alternatively, many users may choose to launch an RStudio-server instance on a cloud platform that already has a local copy of the GBIF data available. 

Data can be downloaded by registering at GBIF portal, or directly from various cloud provider copies, including the AWS GBIF public data catalog,
<https://registry.opendata.aws/gbif/>, or the Microsoft Cloud,
<https://planetarycomputer.microsoft.com/dataset/gbif>,
which include directions for direct download or syncing through various client software.



Once you have downloaded the parquet-formatted GBIF data, 
simply point `gbif_conn()` at the directory containing your parquet files to initialize a connection.
(By default, `gbif_conn()` will look for data in the configurable directory given by `gbif_dir()`).

```{r}
conn <- gbif_conn()
```


The resulting connection can be used with `dplyr::tbl()` to access the full gbif data:

```{r}  
gbif <- tbl(conn, "gbif")
gbif
```

```{r}
colnames(gbif)
```

Now, we can use `dplyr` to perform standard queries: 
for instance, the number of unique species observed by country:


```{r}
gbif %>% select(species, countrycode) %>%
  distinct() %>% 
  count(countrycode)
```


```{r}
growth <- gbif %>% 
  filter(phylum == "Chordata", year > 1990) %>%
  count(class, year) %>% arrange(year)
growth
```

Recall that when using remote data sources in `dplyr`, the data remains in the database (i.e. on disk, not in working RAM).  This is fine for any further operations using `dplyr`/`tidyr` functions which can be translated into SQL.  Using such functions we can usually reduce our resulting table to something much smaller, which can then be pulled into memory in R for further analysis using `collect()`:

```{r}
library(tidyverse)
growth <- collect(growth)

fct_lump_n(growth$class, 6)%>%levels()
top_classes <- growth %>% pull(class) %>% unique() %>% head()

# GBIF: the global bird information facility?
growth %>% 
  mutate(class = fct_lump_n(class, 6)) %>%
  ggplot(aes(year, n, fill=class)) + geom_col() +
  ggtitle("GBIF observations by class")
```



```{r include=FALSE}
Sys.unsetenv("GBIF_HOME")
```


